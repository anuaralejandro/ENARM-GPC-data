#!/usr/bin/env python3
"""
üîÑ SEGUNDO PASE: Buscar GPCs faltantes con estrategias alternativas.

Estrategias:
1. B√∫squeda m√°s flexible (sin site: restriction tan estricta)
2. Validar a√±o en cat√°logo IMSS (solo usar si es del a√±o actual/anterior)
3. B√∫squeda con t√≠tulo simplificado (quitar palabras complejas)
4. B√∫squeda con t√©rminos m√©dicos clave (extraer diagn√≥stico principal)
5. Si NADA funciona, marcar como "no encontrado" definitivo

Requisitos:
- Ejecutar DESPU√âS de find_gpc_links.py (primer pase)
- Lee gpc_links.json y busca solo las faltantes (None en ger_url o grr_url)
- Guarda resultados en el mismo archivo (actualiza)
"""

from __future__ import annotations
import argparse
import csv
import datetime
import difflib
import hashlib
import io
import json
import os
import re
import sys
import time
import warnings
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any

import requests

# Add parent directory to path to import from main script
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

try:
    import pytesseract
    from PIL import Image
    import fitz  # PyMuPDF
    HAS_OCR = True
except ImportError:
    HAS_OCR = False

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)

REPO_ROOT = Path(__file__).resolve().parents[1]
OUT_DIR = REPO_ROOT / "data"
OUT_JSON = OUT_DIR / "gpc_links.json"
OUT_CSV = OUT_DIR / "gpc_links.csv"
OUT_MD = REPO_ROOT / "docs" / "gpc_links_summary.md"
PDF_CACHE_DIR = OUT_DIR / ".pdf_cache"

# Import from main script
from scripts.find_gpc_links import (
    GPCLinkResult,
    SearchProvider,
    GoogleCSEProvider,
    SerperProvider,
    SerpAPIProvider,
    SemanticValidator,
    download_pdf,
    get_domain_priority,
    classify_url_type,
    normalize_text,
    load_imss_catalog,
    write_outputs,
)


def choose_provider() -> SearchProvider:
    """Same as main script but prioritize Google CSE."""
    g_key = os.environ.get("GOOGLE_API_KEY")
    g_cse = os.environ.get("GOOGLE_CSE_ID")
    serpapi_key = os.environ.get("SERPAPI_API_KEY")
    serper_key = os.environ.get("SERPER_API_KEY")
    
    if g_key and g_cse:
        return GoogleCSEProvider(g_key, g_cse)
    if serpapi_key:
        return SerpAPIProvider(serpapi_key)
    if serper_key:
        return SerperProvider(serper_key)
    
    raise RuntimeError("No search provider configured.")


def simplify_title(title: str) -> str:
    """
    Simplifica t√≠tulo para b√∫squeda m√°s flexible.
    
    Ejemplo:
    - "Diagn√≥stico y tratamiento de la neumon√≠a adquirida en la comunidad" 
      ‚Üí "neumon√≠a adquirida comunidad"
    """
    # Quitar palabras comunes
    stop_words = [
        'diagn√≥stico', 'tratamiento', 'manejo', 'prevenci√≥n', 'detecci√≥n',
        'de', 'del', 'la', 'el', 'en', 'por', 'para', 'con', 'y', 'o',
        'a', 'ante', 'bajo', 'cabe', 'desde', 'entre', 'hacia', 'hasta',
        'mediante', 'seg√∫n', 'sin', 'sobre', 'tras'
    ]
    
    words = title.lower().split()
    filtered = [w for w in words if w not in stop_words and len(w) > 3]
    
    return ' '.join(filtered[:5])  # Max 5 palabras clave


def extract_key_medical_terms(title: str) -> List[str]:
    """
    Extrae t√©rminos m√©dicos clave del t√≠tulo.
    
    Ejemplo:
    - "Tratamiento de diabetes mellitus tipo 2" ‚Üí ["diabetes", "mellitus"]
    """
    # Palabras m√©dicas importantes (no stop words)
    medical_pattern = r'\b([a-z√°√©√≠√≥√∫√±]{5,})\b'
    matches = re.findall(medical_pattern, title.lower())
    
    # Filtrar stop words m√©dicas
    stop_medical = ['tratamiento', 'diagn√≥stico', 'manejo', 'prevenci√≥n']
    
    return [m for m in matches if m not in stop_medical][:3]


def get_gpc_year_from_catalog(entry: Dict[str, Any]) -> Optional[int]:
    """
    Extrae a√±o de publicaci√≥n de una entrada del cat√°logo IMSS.
    
    Busca en:
    1. Campo 'year' (si existe)
    2. N√∫mero de GPC (formato: XXX-YY donde YY es a√±o)
    3. URL (buscar /20XX/ o _20XX_)
    """
    # Campo directo
    if 'year' in entry:
        try:
            return int(entry['year'])
        except (ValueError, TypeError):
            pass
    
    # Extraer de n√∫mero GPC (ej: IMSS-031-08 ‚Üí 2008)
    if 'gpc_number' in entry:
        match = re.search(r'-(\d{2})$', entry['gpc_number'])
        if match:
            year_suffix = int(match.group(1))
            # Asumir 20XX si YY > 90, sino 19XX
            return 2000 + year_suffix if year_suffix <= 25 else 1900 + year_suffix
    
    # Extraer de URL
    for url_key in ['ger_url', 'grr_url']:
        url = entry.get(url_key, '')
        if url:
            # Buscar /20XX/ o _20XX_
            match = re.search(r'[/_](20\d{2})[/_]', url)
            if match:
                return int(match.group(1))
    
    return None


def is_recent_gpc(entry: Dict[str, Any], max_age_years: int = 5) -> bool:
    """
    Verifica si un GPC del cat√°logo IMSS es suficientemente reciente.
    
    Args:
        entry: Entrada del cat√°logo IMSS
        max_age_years: M√°xima antig√ºedad permitida (default: 5 a√±os)
    
    Returns: True si es reciente o no se puede determinar a√±o
    """
    year = get_gpc_year_from_catalog(entry)
    
    if year is None:
        # Si no podemos determinar el a√±o, aceptar (beneficio de la duda)
        return True
    
    current_year = datetime.datetime.now().year
    age = current_year - year
    
    return age <= max_age_years


def find_in_imss_catalog_by_year(title: str, imss_catalog: List[Dict[str, Any]], max_age_years: int = 5) -> Optional[Dict[str, Any]]:
    """
    Busca en cat√°logo IMSS filtrando por a√±o.
    
    Args:
        title: T√≠tulo del GPC
        imss_catalog: Cat√°logo IMSS
        max_age_years: M√°xima antig√ºedad (a√±os)
    
    Returns: Mejor match reciente o None
    """
    title_norm = normalize_text(title)
    
    candidates = []
    
    for entry in imss_catalog:
        # Filtrar por a√±o primero
        if not is_recent_gpc(entry, max_age_years):
            continue
        
        entry_title = entry.get('title', '')
        if not entry_title:
            continue
        
        entry_norm = normalize_text(entry_title)
        
        # Similitud
        score = difflib.SequenceMatcher(None, title_norm, entry_norm).ratio()
        
        if score >= 0.65:  # Threshold m√°s bajo para segundo pase
            year = get_gpc_year_from_catalog(entry) or 9999
            candidates.append((score, year, entry))
    
    if not candidates:
        return None
    
    # Ordenar por similitud primero, luego por a√±o (m√°s reciente)
    candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)
    
    return candidates[0][2]


def search_flexible(title: str, doc_type: str, provider: SearchProvider, validator: SemanticValidator) -> Optional[str]:
    """
    üîç B√∫squeda M√ÅS FLEXIBLE para segundo pase.
    
    ‚ö†Ô∏è PRIORIDAD CENETEC SIEMPRE (incluso en segundo pase):
    1. Buscar en CENETEC con estrategias alternativas
    2. Si no hay en CENETEC, buscar en IMSS
    3. Si no hay en IMSS, buscar en cualquier .mx
    4. Validaci√≥n m√°s permisiva (30% threshold)
    """
    doc_keywords = {
        "GER": '"Gu√≠a de Evidencias" OR GER OR ER',
        "GRR": '"Gu√≠a de Referencia" OR GRR OR RR'
    }
    
    # üî• ESTRATEGIA: Buscar en cada dominio por prioridad
    domain_strategies = [
        # PRIORIDAD 1: CENETEC (siempre primero)
        [
            f'{title} {doc_keywords[doc_type]} filetype:pdf site:cenetec-difusion.com',
            f'{simplify_title(title)} {doc_keywords[doc_type]} filetype:pdf site:cenetec-difusion.com',
            f'{" ".join(extract_key_medical_terms(title))} {doc_keywords[doc_type]} filetype:pdf site:cenetec-difusion.com',
            f'{title} {doc_keywords[doc_type]} filetype:pdf site:cenetec.salud.gob.mx',
            f'{simplify_title(title)} {doc_keywords[doc_type]} filetype:pdf site:cenetec.salud.gob.mx',
        ],
        
        # PRIORIDAD 2: IMSS (solo si CENETEC no tiene)
        [
            f'{title} {doc_keywords[doc_type]} filetype:pdf site:imss.gob.mx',
            f'{simplify_title(title)} {doc_keywords[doc_type]} filetype:pdf site:imss.gob.mx',
            f'{" ".join(extract_key_medical_terms(title))} {doc_keywords[doc_type]} filetype:pdf site:imss.gob.mx',
        ],
        
        # PRIORIDAD 3: Cualquier .mx (√∫ltimo recurso)
        [
            f'{title} {doc_keywords[doc_type]} filetype:pdf site:.mx',
            f'{simplify_title(title)} {doc_keywords[doc_type]} filetype:pdf site:.mx',
        ],
    ]
    
    domain_names = ["CENETEC", "IMSS", ".mx"]
    
    # Buscar por prioridad de dominio
    for domain_idx, strategies in enumerate(domain_strategies):
        print(f"  ÔøΩ [{domain_names[domain_idx]}] Probando {len(strategies)} estrategias...")
        
        for i, query in enumerate(strategies, 1):
            try:
                results = provider.search(query, num=10)
                
                if not results:
                    continue
                
                print(f"    [{i}] {len(results)} resultados")
                
                # Validar candidatos (ordenar por prioridad de dominio)
                candidates = []
                
                for result in results:
                    url = result.get("link", "")
                    
                    if not url or not url.lower().endswith(".pdf"):
                        continue
                    
                    # Filtro de dominio permitido
                    if ".mx" not in url.lower() and "cenetec" not in url.lower():
                        continue
                    
                    url_type = classify_url_type(url)
                    if url_type and url_type != doc_type:
                        continue
                    
                    # Descargar y validar
                    pdf_bytes = download_pdf(url)
                    if not pdf_bytes or len(pdf_bytes) < 1000:
                        continue
                    
                    # Validaci√≥n m√°s permisiva (30% threshold)
                    is_valid, confidence, detected_type, title_sim = validator.validate_pdf(
                        pdf_bytes, title, doc_type, url_type
                    )
                    
                    # Umbral m√°s bajo para segundo pase
                    if title_sim >= 0.30 and detected_type == doc_type:
                        domain_priority = get_domain_priority(url)
                        candidates.append((domain_priority, confidence, url))
                
                # Si hay candidatos, retornar el mejor (por dominio primero)
                if candidates:
                    candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)
                    best_priority, best_conf, best_url = candidates[0]
                    print(f"    ‚úÖ ENCONTRADO: {best_url[-60:]} (domain={best_priority}, conf={best_conf:.1%})")
                    return best_url
            
            except Exception as e:
                print(f"    ‚ùå Error estrategia {i}: {e}")
                continue
        
        # Si encontramos algo en este dominio, no buscar en dominios de menor prioridad
        # (pero si no encontramos nada, continuar al siguiente dominio)
    
    return None


def process_missing_gpcs(args) -> None:
    """
    Procesa GPCs faltantes del primer pase.
    """
    # Cargar resultados del primer pase
    if not OUT_JSON.exists():
        print(f"‚ùå No se encontr√≥ {OUT_JSON}")
        print("Ejecuta primero: python scripts/find_gpc_links.py --use-smart-validation")
        return
    
    rows = []
    try:
        existing_data = json.loads(OUT_JSON.read_text(encoding="utf-8"))
        for item in existing_data:
            result = GPCLinkResult(
                title=item.get('title', ''),
                query_ger=item.get('query_ger', ''),
                query_grr=item.get('query_grr', ''),
                ger_url=item.get('ger_url'),
                grr_url=item.get('grr_url'),
                ger_source=item.get('ger_source'),
                grr_source=item.get('grr_source'),
                ger_status=item.get('ger_status'),
                grr_status=item.get('grr_status'),
                ger_confidence=float(item.get('ger_confidence', 0.0) or 0.0),
                grr_confidence=float(item.get('grr_confidence', 0.0) or 0.0),
            )
            rows.append(result)
    except Exception as e:
        print(f"‚ùå Error cargando {OUT_JSON}: {e}")
        return
    
    # Filtrar faltantes
    missing = []
    for row in rows:
        needs_ger = not row.ger_url
        needs_grr = not row.grr_url
        
        if needs_ger or needs_grr:
            missing.append((row, needs_ger, needs_grr))
    
    if not missing:
        print("‚úÖ No hay GPCs faltantes. Todo completo!")
        return
    
    print(f"\nüîç SEGUNDO PASE: {len(missing)} GPCs con documentos faltantes")
    print("=" * 80)
    
    # Inicializar provider y validator
    provider = choose_provider()
    
    validator = SemanticValidator(
        args.embedding_model,
        args.embedding_device,
        args.embedding_batch_size,
        enable_classification=True
    )
    
    if not validator.ensure_loaded():
        print("‚ùå No se pudo cargar el modelo de validaci√≥n")
        return
    
    print(f"‚úÖ Modelo cargado en {validator._actual_device}\n")
    
    # Cargar cat√°logo IMSS
    imss_catalog = load_imss_catalog()
    if imss_catalog:
        print(f"üì¶ Cat√°logo IMSS: {len(imss_catalog)} GPCs disponibles\n")
    
    # Procesar faltantes
    updated_count = 0
    
    for idx, (row, needs_ger, needs_grr) in enumerate(missing, 1):
        print(f"\n[{idx}/{len(missing)}] {row.title}")
        
        # üî• PRIORIDAD CENETEC SIEMPRE: Buscar en web primero
        # Si encontramos en CENETEC, nunca usar IMSS catalog
        
        if needs_ger:
            print(f"  üîç B√∫squeda flexible: GER")
            url = search_flexible(row.title, "GER", provider, validator)
            if url:
                row.ger_url = url
                row.ger_source = "flexible_search"
                row.ger_confidence = 65.0
                updated_count += 1
                needs_ger = False
            else:
                print(f"    ‚ùå GER no encontrado en web")
        
        if needs_grr:
            print(f"  üîç B√∫squeda flexible: GRR")
            url = search_flexible(row.title, "GRR", provider, validator)
            if url:
                row.grr_url = url
                row.grr_source = "flexible_search"
                row.grr_confidence = 65.0
                updated_count += 1
                needs_grr = False
            else:
                print(f"    ‚ùå GRR no encontrado en web")
        
        # FALLBACK: IMSS catalog SOLO si no encontramos en web (solo si es reciente)
        if (needs_ger or needs_grr) and imss_catalog:
            imss_match = find_in_imss_catalog_by_year(row.title, imss_catalog, max_age_years=args.max_age)
            
            if imss_match:
                year = get_gpc_year_from_catalog(imss_match)
                year_str = f"a√±o {year}" if year else "a√±o desconocido"
                print(f"  üìö IMSS catalog fallback ({year_str})")
                
                if needs_ger and imss_match.get('ger_url'):
                    row.ger_url = imss_match['ger_url']
                    row.ger_source = "IMSS_catalog_fallback"
                    row.ger_confidence = 70.0
                    print(f"    ‚úÖ GER: {row.ger_url[-50:]}")
                    updated_count += 1
                    needs_ger = False
                
                if needs_grr and imss_match.get('grr_url'):
                    row.grr_url = imss_match['grr_url']
                    row.grr_source = "IMSS_catalog_fallback"
                    row.grr_confidence = 70.0
                    print(f"    ‚úÖ GRR: {row.grr_url[-50:]}")
                    updated_count += 1
                    needs_grr = False
        
        # Resultado final
        if needs_ger and needs_grr:
            print(f"    ‚ùå Ambos faltantes (b√∫squeda exhaustiva completada)")
        elif needs_ger:
            print(f"    ‚ö†Ô∏è  Solo GER faltante")
        elif needs_grr:
            print(f"    ‚ö†Ô∏è  Solo GRR faltante")
        
        # Guardar progreso incremental
        write_outputs(rows, incremental=True)
        
        # Sleep para no saturar API
        time.sleep(args.sleep)
    
    print(f"\n{'=' * 80}")
    print(f"‚úÖ SEGUNDO PASE COMPLETADO")
    print(f"üìä {updated_count} documentos adicionales encontrados")
    print(f"üíæ Resultados guardados en: {OUT_JSON}")


def main(argv: List[str]) -> int:
    parser = argparse.ArgumentParser(
        description="üîÑ Segundo pase: buscar GPCs faltantes con estrategias alternativas"
    )
    parser.add_argument("--sleep", type=float, default=1.0, help="Segundos entre b√∫squedas")
    parser.add_argument("--max-age", type=int, default=5, help="M√°xima antig√ºedad de GPCs IMSS (a√±os)")
    parser.add_argument("--embedding-model", default="paraphrase-multilingual-mpnet-base-v2")
    parser.add_argument("--embedding-device", default="auto", choices=["auto", "cpu", "cuda"])
    parser.add_argument("--embedding-batch-size", type=int, default=32)
    
    args = parser.parse_args(argv)
    
    process_missing_gpcs(args)
    
    return 0


if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
